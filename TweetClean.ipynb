{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IRHeHGcFygCN"
   },
   "source": [
    "# **0.0. Dataset:** Json files are parse and clean in R. Profession, tweets text and Emotions (extracted using syuzhet package of R) are in our dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ek_W6rc1xS8b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "xQ-t8z0HxfGe",
    "outputId": "9a1685f7-d184-4239-c347-14e9b42d626f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profession</th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News</td>\n",
       "      <td>Ecuador confirms its first case of new #corona...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News</td>\n",
       "      <td>First #coronavirus death on US soil confirmed,...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>#Coronavirus UPDATE: S. Korea reports 376 more...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News</td>\n",
       "      <td>China reports 35 more #coronavirus deaths, 573...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News</td>\n",
       "      <td>[PHOTO] Near-empty airline counters further cr...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Profession  ... positive\n",
       "0       News  ...        0\n",
       "1       News  ...        2\n",
       "2       News  ...        0\n",
       "3       News  ...        0\n",
       "4       News  ...        0\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"NLP_project_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woc8_gAK3tnT"
   },
   "source": [
    "# **1. Text cleaning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqoToidq0Ybh"
   },
   "outputs": [],
   "source": [
    "#Remove urls\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_urls(text))    \n",
    "\n",
    "#Lower case\n",
    "df[\"text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HeUQVN1k3Gre"
   },
   "outputs": [],
   "source": [
    "EMOTICONS = {\n",
    "    u\":‑\\)\":\"Happy face or smiley\",\n",
    "    u\":\\)\":\"Happy face or smiley\",\n",
    "    u\":-\\]\":\"Happy face or smiley\",\n",
    "    u\":\\]\":\"Happy face or smiley\",\n",
    "    u\":-3\":\"Happy face smiley\",\n",
    "    u\":3\":\"Happy face smiley\",\n",
    "    u\":->\":\"Happy face smiley\",\n",
    "    u\":>\":\"Happy face smiley\",\n",
    "    u\"8-\\)\":\"Happy face smiley\",\n",
    "    u\":o\\)\":\"Happy face smiley\",\n",
    "    u\":-\\}\":\"Happy face smiley\",\n",
    "    u\":\\}\":\"Happy face smiley\",\n",
    "    u\":-\\)\":\"Happy face smiley\",\n",
    "    u\":c\\)\":\"Happy face smiley\",\n",
    "    u\":\\^\\)\":\"Happy face smiley\",\n",
    "    u\"=\\]\":\"Happy face smiley\",\n",
    "    u\"=\\)\":\"Happy face smiley\",\n",
    "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":-\\)\\)\":\"Very happy\",\n",
    "    u\":‑\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
    "    u\":@\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":'‑\\(\":\"Crying\",\n",
    "    u\":'\\(\":\"Crying\",\n",
    "    u\":'‑\\)\":\"Tears of happiness\",\n",
    "    u\":'\\)\":\"Tears of happiness\",\n",
    "    u\"D‑':\":\"Horror\",\n",
    "    u\"D:<\":\"Disgust\",\n",
    "    u\"D:\":\"Sadness\",\n",
    "    u\"D8\":\"Great dismay\",\n",
    "    u\"D;\":\"Great dismay\",\n",
    "    u\"D=\":\"Great dismay\",\n",
    "    u\"DX\":\"Great dismay\",\n",
    "    u\":‑O\":\"Surprise\",\n",
    "    u\":O\":\"Surprise\",\n",
    "    u\":‑o\":\"Surprise\",\n",
    "    u\":o\":\"Surprise\",\n",
    "    u\":-0\":\"Shock\",\n",
    "    u\"8‑0\":\"Yawn\",\n",
    "    u\">:O\":\"Yawn\",\n",
    "    u\":-\\*\":\"Kiss\",\n",
    "    u\":\\*\":\"Kiss\",\n",
    "    u\":X\":\"Kiss\",\n",
    "    u\";‑\\)\":\"Wink or smirk\",\n",
    "    u\";\\)\":\"Wink or smirk\",\n",
    "    u\"\\*-\\)\":\"Wink or smirk\",\n",
    "    u\"\\*\\)\":\"Wink or smirk\",\n",
    "    u\";‑\\]\":\"Wink or smirk\",\n",
    "    u\";\\]\":\"Wink or smirk\",\n",
    "    u\";\\^\\)\":\"Wink or smirk\",\n",
    "    u\":‑,\":\"Wink or smirk\",\n",
    "    u\";D\":\"Wink or smirk\",\n",
    "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":‑\\|\":\"Straight face\",\n",
    "    u\":\\|\":\"Straight face\",\n",
    "    u\":$\":\"Embarrassed or blushing\",\n",
    "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\"O:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
    "    u\"0:3\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
    "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
    "    u\">:‑\\)\":\"Evil or devilish\",\n",
    "    u\">:\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:‑\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:\\)\":\"Evil or devilish\",\n",
    "    u\"3:‑\\)\":\"Evil or devilish\",\n",
    "    u\"3:\\)\":\"Evil or devilish\",\n",
    "    u\">;\\)\":\"Evil or devilish\",\n",
    "    u\"\\|;‑\\)\":\"Cool\",\n",
    "    u\"\\|‑O\":\"Bored\",\n",
    "    u\":‑J\":\"Tongue-in-cheek\",\n",
    "    u\"#‑\\)\":\"Party all night\",\n",
    "    u\"%‑\\)\":\"Drunk or confused\",\n",
    "    u\"%\\)\":\"Drunk or confused\",\n",
    "    u\":-###..\":\"Being sick\",\n",
    "    u\":###..\":\"Being sick\",\n",
    "    u\"<:‑\\|\":\"Dump\",\n",
    "    u\"\\(>_<\\)\":\"Troubled\",\n",
    "    u\"\\(>_<\\)>\":\"Troubled\",\n",
    "    u\"\\(';'\\)\":\"Baby\",\n",
    "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
    "    u\"\\(\\^_-\\)\":\"Wink\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
    "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
    "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
    "    u\"\\^_\\^\":\"Joyful\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
    "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
    "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;_;\":\"Sad of Crying\",\n",
    "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
    "    u\";_;\":\"Sad or Crying\",\n",
    "    u\";-;\":\"Sad or Crying\",\n",
    "    u\";n;\":\"Sad or Crying\",\n",
    "    u\";;\":\"Sad or Crying\",\n",
    "    u\"Q\\.Q\":\"Sad or Crying\",\n",
    "    u\"T\\.T\":\"Sad or Crying\",\n",
    "    u\"QQ\":\"Sad or Crying\",\n",
    "    u\"Q_Q\":\"Sad or Crying\",\n",
    "    u\"\\(-\\.-\\)\":\"Shame\",\n",
    "    u\"\\(-_-\\)\":\"Shame\",\n",
    "    u\"\\(一一\\)\":\"Shame\",\n",
    "    u\"\\(；一_一\\)\":\"Shame\",\n",
    "    u\"\\(=_=\\)\":\"Tired\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
    "    u\"=_\\^=\t\":\"cat\",\n",
    "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
    "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
    "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
    "    u\"\\(\\・\\・?\":\"Confusion\",\n",
    "    u\"\\(?_?\\)\":\"Confusion\",\n",
    "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
    "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
    "    u\"\\^/\\^\":\"Normal Laugh\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
    "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
    "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
    "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
    "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
    "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
    "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
    "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
    "    u'\\(-\"-\\)':\"Worried\",\n",
    "    u\"\\(ーー;\\)\":\"Worried\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
    "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
    "    u\":O o_O\":\"Surprised\",\n",
    "    u\"o_0\":\"Surprised\",\n",
    "    u\"o\\.O\":\"Surpised\",\n",
    "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
    "    u\"oO\":\"Surprised\",\n",
    "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
    "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHaLfWP93Hho"
   },
   "outputs": [],
   "source": [
    "#Remove Emojis\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "#Remove Emoticons\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"lxml\").text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Remove numbers\n",
    "df[\"text\"] = df[\"text\"].str.replace(r\"#(\\w+)\", '')\n",
    "df[\"text\"] = df[\"text\"].str.replace(r\"@(\\w+)\", '')\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_emoji(text))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_emoticons(text))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_html(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbDUPR3K3R5U"
   },
   "outputs": [],
   "source": [
    "##Url: https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
    "\n",
    "import csv\n",
    "#Chat Words Conversion\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its Abbreviation in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    return ' '.join(user_string)\n",
    "    \n",
    "    \n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: translator(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFvD-M9u3Sw8"
   },
   "outputs": [],
   "source": [
    "#Remove punctuations\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "\n",
    "#Remove stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "#Remove numbers\n",
    "df[\"text\"] = df[\"text\"].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "colab_type": "code",
    "id": "Oe5nPeMK3VVr",
    "outputId": "6ceb4cec-ab51-42a4-aeb2-d9e794515167"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-12514abb7094>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Spelling correction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspellchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorrect_spellings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spellchecker/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0mspellchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellchecker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spellchecker/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionaryIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_detect_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'indexer'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Spelling correction\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: correct_spellings(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWGRbr5L3YAC"
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "aBaMSko83dkB",
    "outputId": "3ea7c2b9-f039-4d20-ab86-e71fc560e669"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case', 5251),\n",
       " ('new', 2568),\n",
       " ('test', 2360),\n",
       " ('death', 2270),\n",
       " ('say', 2004),\n",
       " ('people', 1783),\n",
       " ('health', 1754),\n",
       " ('report', 1742),\n",
       " ('country', 1671),\n",
       " ('total', 1604)]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove most common words\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"text\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "fQghlu033lvd",
    "outputId": "021a01dc-6689-4c2b-d288-3558c13fbbda"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profession</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News</td>\n",
       "      <td>ecuador confirms first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News</td>\n",
       "      <td>first u soil confirm trump call calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>update korea coronavirus via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News</td>\n",
       "      <td>china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News</td>\n",
       "      <td>photo nearempty airline counter cripple reduce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Profession                                               text\n",
       "0       News                             ecuador confirms first\n",
       "1       News               first u soil confirm trump call calm\n",
       "2       News                       update korea coronavirus via\n",
       "3       News                                              china\n",
       "4       News  photo nearempty airline counter cripple reduce..."
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_freqwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rare words\n",
    "n_rare_words = 10\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_rarewords(text))\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
